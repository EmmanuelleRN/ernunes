---
title: Conditional Probability
author: Emmanuelle R. Nunes
date: September 19 2021
slug: September 19 2021-introduction-to-probability
categories: 
  - Introduction
  - Statistics
  - Probability
  - Bayes theorem
tags: 
  - statistics
  - introduction
  - probability
  - bayes
  - conditional probability
  - independence
subtitle: and Independence
summary: ''
authors: []
lastmod: '2021-09-19T15:52:58+01:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<p>For any two events A and B, with <span class="math inline">\(\mathbb{P}(B) &gt; 0\)</span>, we define the conditional probability of A given B (<span class="math inline">\(\mathbb{P}(A|B)\)</span>) as</p>
<p><span class="math display">\[\mathbb{P}(A|B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}\]</span></p>
<p><strong>Example:</strong> We throw a dice once, and it lands on an even number. What is the probability of this number is 2?</p>
<p>We can define A as “dice shows number 2”, and B as “dice result is even”, we have then that A = {2}, B = {2, 4, 6} and <span class="math inline">\(A \cap B = {2}\)</span>, then</p>
<p><span class="math display">\[\mathbb{P}(A|B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)} = \frac{1}{3}\]</span></p>
<div id="independence" class="section level2">
<h2>Independence</h2>
<p>Independence between two or more variables can help us to ease some of our analysis. We say that two events A and B are independent if and only if <span class="math inline">\(\mathbb{P}(A \cap B) = \mathbb{P}(A) \mathbb{P}(B)\)</span>, i.e., two events are independent if the incidence of one event does not affect the probability of the other event. If the incidence of one event does affect the probability of the other event, then the events are dependent.</p>
<p>The concept of conditional probability is closely related to independent events, and we can define independence between events using conditional probability.</p>
<p>Two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent if:</p>
<p><span class="math display">\[\mathbb{P}(A\mid B) = \mathbb{P}(A \mid B^c) \text{ and } \mathbb{P}(B\mid A) = \mathbb{P}(B\mid A^c)\]</span></p>
<p>Two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are dependent if :</p>
<p><span class="math display">\[\mathbb{P}(A\mid B) \neq \mathbb{P}(A \mid B^c) \text{ or } \mathbb{P}(B\mid A) \neq \mathbb{P}(B\mid A^c)\]</span></p>
<p><strong>Example:</strong> Two fair 6-sided dice are rolled, one red and one blue.
<span class="math inline">\(A\)</span> = the red die’s result is 3.
<span class="math inline">\(B\)</span> = the blue die’s result is 4.
<span class="math inline">\(C\)</span> = the event that the sum of the rolls is 7.
Are <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span> mutually independent?</p>
<p><span class="math display">\[\mathbb{P}(A|B) = \frac{1}{6} = \mathbb{P}(A \mid B^c) \implies \text{A and B are independent}\]</span></p>
<p><span class="math display">\[\mathbb{P}(A|C) = \frac{1}{6} = \mathbb{P}(A \mid C^c) \implies \text{A and C are independent}\]</span></p>
<p><span class="math display">\[\mathbb{P}(B|C) = \frac{1}{6} = \mathbb{P}(B \mid C^c) \implies \text{B and C are independent}\]</span></p>
<p>Theses events are pairwise independent. However, in order for all three events to be mutually independent, each event must be independent with each intersection of the other events.</p>
<p><span class="math display">\[\mathbb{P}\left(A\mid (B\cap C)\right) = 1 \text{ and } P\left(A\mid (B\cap C)^c\right)=\dfrac{1}{7}\]</span></p>
<p>These are not equal, <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, and <span class="math inline">\(C\)</span> are mutually dependent.</p>
</div>
<div id="bayes-theorem" class="section level1">
<h1>Bayes theorem</h1>
<p>One of the most essential conditional probability entities is the one from the Bayes theorem. The simplified formula is given by:</p>
<p><span class="math display">\[\mathbb{P}(A \mid B) = \frac{\mathbb{P}(B \mid A)\mathbb{P}(A)}{\mathbb{P}(B)}\]</span></p>
<p>A bit of the history of this formula and of Bayes itself. In the 1700s, the theory of Probability was consolidating itself, and we could only calculate probabilities of effects knowing the cause,</p>
<p><span class="math display">\[\mathbb{P}(\text{effect} \mid \text{cause})\]</span></p>
<p>The British reverend Thomas Bayes wanted to know how to infer causes from effects, i.e., he was interested in what we call the inverse probability. His question could be summarized as ‘How could we learn the probability of a future event occurring if we only knew how many times it had happened or not in the past?’, so he was interested at</p>
<p><span class="math display">\[\mathbb{P}(\text{cause} \mid \text{effect})\]</span></p>
<p>To illustrate this, he created an experiment. In his experiment, Bayes asked his assistant to drop the ball on a table when his back was turned to the table.</p>
<p>The table is flat, so the ball has just as much chance of landing at any one place on the table as anywhere else. Now Bayes has to figure out where the ball is, without looking.</p>
<center>
<img src="www/bayes_experiment.jpg"/>
</center>
<p>He then asks his assistant to throw another ball on the table and to tell him if the new ball is to the left or right to the first ball. Where he could conclude:</p>
<ul>
<li>If the new ball landed to the left of the first ball, then the first ball is more likely to be on the right side of the table than the left side.</li>
</ul>
<p>He asks his assistant to throw another ball. If it again lands to the left of the first ball, then the first ball is even more likely than before to be on the right side of the table. And so on.</p>
<p>The more balls we throw, the more we can narrow down where the first ball probably is. Each new piece of information constrains the area where the first ball probably is.</p>
<blockquote>
<p>This is a learning process!</p>
</blockquote>
<p>Before having an observation, any position is as possible as any other. Bayes system shows us that an initial belief with data gives us an improved view.</p>
<ul>
<li>The initial belief is what we call prior</li>
<li>The improved opinion is the posterior</li>
</ul>
<p>In each new round of belief updating, the most recent posterior becomes the prior for the new calculation.</p>
<p>Bayes never published his discovery, his friend Richard Price found it among his notes after Bayes’ death in 1761, re-edited it, and published it. Bayes also did not create modern concepts such as Bayesian statistics or Bayesian inference. These were introduced in the 1950s.</p>
<p>There were two criticisms to Bayes’ system:</p>
<ol style="list-style-type: decimal">
<li>Mathematicians disapproved the use of guessing in rigorous mathematics.<br />
</li>
<li>Bayes said that if he didn’t know what guess to make, he’d just assign all possibilities equal.</li>
</ol>
<p>Pierre-Simon Laplace, a brilliant young mathematician, rediscovered Bayes’ mechanism and published it in 1774. Laplace stated that the probability of a cause (given an event) is proportional to the probability of the event (given its cause). The formula and all concepts regarding Bayes theorem and eventually Bayesian Statistic are due to the work of Laplace.</p>
<p>We can generalise the results of the theorem by assuming that <span class="math inline">\(\{C_1, C_2, …, C_n\}\)</span> are partitions of the sample space <span class="math inline">\(\Omega\)</span>, i.e.,</p>
<p><span class="math display">\[C_i \cap C_j = \varnothing \text{ if } i=j\]</span>
<span class="math display">\[C_1 \cup C_2 \cup … \cup C_n = \Omega\]</span></p>
<p>If we consider some event <span class="math inline">\(A\)</span>, where <span class="math inline">\(A \in \Omega\)</span> with know probabilities <span class="math inline">\(\mathbb{P}(C_i)\)</span> and <span class="math inline">\(\mathbb{P}(A \mid C_i)\)</span> we result in the full version of the theorem. The picture below demonstrates this.</p>
<center>
<img src="www/full_theorem.PNG"/>
</center>
<p>The full version of the theorem is the probability of occurrence of an event <span class="math inline">\(C_i\)</span> given that the event A has happened is given by:</p>
<p><span class="math display">\[\mathbb{P}(C_i \mid A) = \frac{\mathbb{P}(C_i)\mathbb{P}(A \mid C_i)}{\sum_{j=1}^n\mathbb{P}(C_j)\mathbb{P}(A \mid C_j)}, \forall i=1,2,…, n\]</span></p>
<p>We can think of <span class="math inline">\(C_1, C_2, …, C_n\)</span> as a set of hypotheses, where only one of them is true. Given that A occurred, the initial probability of <span class="math inline">\(C_i\)</span> is is updated by the extra knowledge.</p>
<p>A very famous problem that we can solve using Bayes theorem is the <strong>Monty Hall</strong>. The Monty Hall problem is based on the American television game show <em>Let’s Make a Deal</em> and named after its host, Monty Hall.</p>
<p>Suppose you’re on a game show, and you’re given the choice of three doors: Behind one door is a car; behind the others, goats.</p>
<center>
<img src="www/monty1.png"/>
</center>
<p>You’ll get the prize behind the door you pick, but you don’t know which prize is behind which door. Obviously you want the car!</p>
<p>So you pick, for example, door 1. Before opening your door, the host, who knows what’s behind the doors, opens another door, say No. 3, which has a goat.</p>
<center>
<img src="featured.png"/>
</center>
<p>Note that Monty needs to open a door and that he will never open the door that has the car.</p>
<p>He then asks you if you’d like to change your guess. Should you?</p>
<p>A lot of people think that it doesn’t matter if I change my guess or not. There are 2 doors, so the odds of winning the car with each is 50%. Unfortunately, that’s 100% wrong. Using Bayes theorem, we can prove why it is in your best interest to <em>change doors</em>.</p>
<p>Assuming that we picked door A and Monty opened door C, we need to calculate 2 posteriors.</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\mathbb{P}(\text{door}=A|\text{opens}=C)\)</span>, the probability of <span class="math inline">\(A\)</span> is correct if Monty opened C,</li>
<li><span class="math inline">\(\mathbb{P}(\text{door}=B|\text{opens}=C)\)</span>, the probability of <span class="math inline">\(B\)</span> is correct if Monty opened <span class="math inline">\(C\)</span>.</li>
</ol>
<div id="prior-pa" class="section level3">
<h3>Prior: P(A)</h3>
<p>The probability of any door being correct before we pick a door is <span class="math inline">\(\frac{1}{3}\)</span>.</p>
<ul>
<li><span class="math inline">\(\mathbb{P}(\text{door}=A) = \frac{1}{3}\)</span>
<ul>
<li>Prior probability that door A contains the car</li>
</ul></li>
<li><span class="math inline">\(\mathbb{P}(\text{door}=B) = \frac{1}{3}\)</span>
<ul>
<li>Prior probability that door B contains the car</li>
</ul></li>
</ul>
</div>
<div id="likelihood-pba" class="section level3">
<h3>Likelihood: P(B|A)</h3>
<p>If the car is actually behind door A, then Monty can open door B or C. So the probability of opening either is <span class="math inline">\(\frac{1}{2}\)</span>.</p>
<p>If the car is actually behind door B then Monty can only open door C as he can’t open A, the door we picked and also can’t open door B because it has the car behind it.</p>
<ul>
<li><span class="math inline">\(\mathbb{P}(\text{opens}=C \mid \text{door}=A) = \frac{1}{2}\)</span>
<ul>
<li>Likelihood Monty opened door C if door A is correct</li>
</ul></li>
<li><span class="math inline">\(\mathbb{P}(\text{opens}=C \mid \text{door}=B) = 1\)</span>
<ul>
<li>Likelihood Monty opened door C if door B is correct</li>
</ul></li>
</ul>
</div>
<div id="pb" class="section level3">
<h3>P(B)</h3>
<p>We can calculate <span class="math inline">\(\mathbb{P}(C)\)</span> as:</p>
<p><span class="math display">\[\begin{align*}
\mathbb{P}(\text{door} = C) &amp; = \mathbb{P}(C \mid A) \mathbb{P}(A) + \mathbb{P}(C\mid B)\mathbb{P}(B) \\
&amp; =  \frac{1}{2}\times \frac{1}{3}+1\times \frac{1}{3} = \frac{1}{2}
\end{align*}\]</span></p>
</div>
<div id="posterior-pab" class="section level3">
<h3>Posterior: P(A|B)</h3>
<p>Now we do the remaining math:</p>
<p><span class="math display">\[\begin{align*}
\mathbb{P}(\text{door}=A \mid \text{opens}=C) &amp; = \frac{\mathbb{P}(\text{open}=C \mid \text{door} = A)\mathbb{P}(\text{door=A})}{\mathbb{P}(\text{open=C})} \\
&amp;= \frac{\frac{1}{2}\times \frac{1}{3}}{\frac{1}{2}} = \frac{1}{3}
\end{align*}\]</span></p>
<p>and</p>
<p><span class="math display">\[\begin{align*}\mathbb{P}(\text{door}=B \mid \text{opens}=C) &amp;= \frac{\mathbb{P}(\text{open}=C \mid \text{door} = B)\mathbb{P}(\text{door=B})}{\mathbb{P}(\text{open=C})} \\
&amp;= \frac{1 \times \frac{1}{3}}{\frac{1}{2}} = \frac{2}{3}
\end{align*}\]</span></p>
<p>This leaves us with a with a higher probability of winning if we change doors after Monty opens a door.</p>
</div>
</div>
